{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "from keras.datasets import mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67dbe35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    The main object we're going to use accross this notebook\n",
    "    It's a neural network that takes as input a list of \n",
    "    layers nodes\n",
    "    \n",
    "    Ex: [2, 3, 1] is a 3 layer network, with 2 neurons of input, 3 neurons \n",
    "    in the hidden layer and 1 for the output layer\n",
    "    \n",
    "    Supposedly it can take more than just 3 layers but I didnt test it\n",
    "    \n",
    "    It initializes an object with the proper weights, biases, activations and z\n",
    "    based on the layers list. It also has the layers list and the number of layers\n",
    "    \n",
    "    The weights and biases initialized following a Gaussian of mean 1\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list):        \n",
    "        np.random.seed(42)        \n",
    "        b = []\n",
    "        w = []\n",
    "        a = []\n",
    "        z = []\n",
    "        for l in range(0, len(layers)):\n",
    "            # skipping one layer for the weights and biases\n",
    "            if (l+1) < len(layers):\n",
    "                b.append(np.random.normal(loc=0, scale=1,size=layers[l+1]))\n",
    "                w.append(np.random.normal(loc=0,scale=1,size=[layers[l],layers[l+1]]))\n",
    "            a.append(np.zeros(layers[l]))\n",
    "            z.append(np.zeros(layers[l]))\n",
    "    \n",
    "        # b[i][j] -> \"i\" is which layer, \"j\" which neuron\n",
    "        # w[i][j][k] -> \"i\" is which layer, \"j\" which neuron of the first layer, \"k\" which neuron of the second layer\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "        self.nLayers = len(layers)\n",
    "        self.layers = layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e54ff0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(n: float):\n",
    "    return max(0, n)\n",
    "\n",
    "\n",
    "def ReLU_derivative(n: float):\n",
    "    \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "    n = np.where(n > 0, 1, 0)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "378ffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n: float):\n",
    "    return 1.0/(1.0+np.exp(-n))\n",
    "\n",
    "def sigmoid_derivative(n: float):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(n)*(1-sigmoid(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ea48a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(net: Network) -> Network:\n",
    "    \"\"\"\n",
    "    Feedforwading the activations to the next layer\n",
    "    \n",
    "    It will take as input the network already with the input image as the activation \n",
    "    on the first layer and then feedforward to the next layrse\n",
    "    \n",
    "    It returns the network with all the activations set\n",
    "    \"\"\"\n",
    "    \n",
    "    # resetting the activations as to not take any info from the activation of\n",
    "    # the previous number while maintanin the first activation\n",
    "    for i in range(1, net.nLayers):\n",
    "        net.z[i] = np.zeros(net.layers[i])\n",
    "        net.a[i] = np.zeros(net.layers[i])\n",
    "    for l in range(0, net.nLayers-1):\n",
    "        for receivingNeuron in range(net.layers[l+1]):\n",
    "            for givingNeuron in range(net.layers[l]):\n",
    "                net.z[l+1][receivingNeuron] += net.a[l][givingNeuron] * net.w[l][givingNeuron][receivingNeuron]\n",
    "            net.z[l+1][receivingNeuron] += net.b[l][receivingNeuron]\n",
    "            net.a[l+1][receivingNeuron] = sigmoid(net.z[l+1][receivingNeuron])\n",
    "\n",
    "            \n",
    "    return net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10120406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setInput(net: Network, MNISTnumber):\n",
    "    \"\"\"\n",
    "    Inputs the MNIST number into the network, since the number is a 28x28 matrix, \n",
    "    we transform it into a 784 array\n",
    "    \n",
    "    We also scale the pixels as to be between 0 and 1 for the sigmoid function \n",
    "    instead of 0 and 255\n",
    "    \n",
    "    Returns the network with the proper activations on all layers since it pass \n",
    "    through the feedforward step\n",
    "    \"\"\"\n",
    "    numberArr = np.asarray(MNISTnumber).flatten()\n",
    "    # scaling the array so that the range is between 0 and 1\n",
    "    numberArr = np.interp(numberArr, (numberArr.min(), numberArr.max()), (0, 1))\n",
    "    for i in range(net.layers[0]):\n",
    "        net.z[0][i] = numberArr[i]\n",
    "        net.a[0][i] = numberArr[i]\n",
    "    net = feedForward(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d026ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNetwork(net: Network, test_X, test_y, nTests: int):\n",
    "    \"\"\"\n",
    "    A function to test our network\n",
    "    \n",
    "    It returns the overall accuracy and the numbers our network guessed\n",
    "    \"\"\"\n",
    "    \n",
    "    correctOutput = 0\n",
    "    X = test_X[:nTests]\n",
    "    y = test_y[:nTests]\n",
    "    outputs = np.zeros(10)\n",
    "    for i in range(nTests):\n",
    "        net = setInput(net, X[i])\n",
    "        networkOutput = np.argmax(net.a[-1])\n",
    "        outputs[networkOutput] += 1\n",
    "        #print(f\"number: {y[i]}, networkOutput: {networkOutput}, activations: {net.a[-1]}\")\n",
    "        if y[i] == networkOutput:\n",
    "            correctOutput += 1\n",
    "    acc = correctOutput/nTests\n",
    "    return acc, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d4f2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(net: Network, train_X, train_y, test_X, test_y, batchSize: int, learningRates: list, epochs: int):\n",
    "    \"\"\"\n",
    "    A function to perform a gridSearch in order to find the best learningRates\n",
    "\n",
    "    It takes as input the network, the training images of MNIST, the training labels,\n",
    "    the test images, the test labels, the batchSize for SGD,\n",
    "    a list of learningRates as to find the best inside the list\n",
    "    the number of epochs to perform SGD\n",
    "    \n",
    "    \n",
    "    It returns the best network accross all learning rates list\n",
    "    \"\"\"\n",
    "    bestNet = net\n",
    "    bestAcc = 0\n",
    "    for eta in learningRates:\n",
    "        # resetting the network\n",
    "        net = Network([784,30,10])\n",
    "        net = SGD(net, train_X, train_y, batchSize=batchSize, nEpochs=epochs, learningRate=eta)\n",
    "        acc, outputs = testNetwork(net, test_X, test_y, batchSize) \n",
    "        if acc > bestAcc:\n",
    "            bestNet = net\n",
    "            bestAcc = acc\n",
    "    return bestNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491dbfe",
   "metadata": {},
   "source": [
    "The list below is all equations that were used to compute the erros and then propagate through the network:\n",
    "\n",
    "To calculate the error on the last layer: \n",
    "$$\\delta^L = (a^L - y)\\odot \\sigma'(z^L)$$\n",
    "\n",
    "To calculate the error on the other layers:\n",
    "$$\\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$$\n",
    "\n",
    "To repass the error to the bias: \n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
    "\n",
    "To repass the error to the weights:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b397ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(net: Network, y) -> Network:\n",
    "    \"\"\"\n",
    "    The backpropagation step: first we calculate the error on the last layer, \n",
    "    then we pass to the previous layers all the while applying the error \n",
    "    to the weights and biases. Here we used Sum of Squared Residuals as our cost function\n",
    "    \n",
    "    Example on a 3 layer network: We calculate the error on the last layer, \n",
    "    apply it to the last layer's weights and biases, and then calculate the \n",
    "    error on the next layer, propagate to the weights and biases and it's done\n",
    "    \n",
    "    It takes as input the network and the label of the number the network was activated on\n",
    "    \n",
    "    It returns the modifications to the weights and biases (nablaW and nablaB) \n",
    "    the network should have\n",
    "    \"\"\"\n",
    "    layers = net.layers\n",
    "    nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "    nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "    delta = np.zeros(10) # 10 because its the possible number of outputs\n",
    "    for j in range(net.layers[-1]):\n",
    "        if y == j:\n",
    "            delta[j] += (net.a[-1][j] - 1)*sigmoid_derivative(net.z[-1][j])\n",
    "        else:\n",
    "            delta[j] += (net.a[-1][j] - 0)*sigmoid_derivative(net.z[-1][j])\n",
    "    for l in range(net.nLayers-1, 0, -1):\n",
    "        #nablaB and nablaW have -1 because they only have 2 layers instead of 3\n",
    "        nablaB[l-1] = delta\n",
    "                \n",
    "        for j in range(layers[l]):\n",
    "            for k in range(layers[l-1]):\n",
    "                nablaW[l-1][k][j] += net.a[l-1][k]*delta[j]\n",
    "        \n",
    "        # finding the error one layer behind\n",
    "        # in the book it needs a transpose because its weight[layer][receivingNeuron][givingNeuron]\n",
    "        # but my implementation uses weight[layer][givingNeuron][receivingNeuron] so it's not necessary\n",
    "        delta = (np.dot(net.w[l-1], delta))*sigmoid_derivative(net.z[l-1])\n",
    "        \n",
    "    return nablaB, nablaW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8536b0c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(net: Network, X: list, y: list, batchSize: int, nEpochs: int, learningRate) -> Network:\n",
    "    \"\"\"\n",
    "    Implementation of Stochastic Gradient Descent\n",
    "    \n",
    "    It takes as input the network, the MNIST dataset, the MNIST labels of the dataset, \n",
    "    the size of the batch to do gradient descent, the number of epochs it should run,\n",
    "    and the learning rate eta (I found the best eta to be in the order of 10s)\n",
    "    \n",
    "    It returns the best trained network in all epochs\n",
    "    \"\"\"\n",
    "    bestAcc = 0\n",
    "    bestEpoch = 0\n",
    "    bestNet = net\n",
    "    for epoch in range(nEpochs):\n",
    "        batch = rd.sample(range(len(X)), batchSize)\n",
    "        nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "        nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "        for i in batch:\n",
    "            net = setInput(net, X[i])\n",
    "            # finding what should be modified based on this particular example\n",
    "            deltaNablaB, deltaNablaW = backProp(net, y[i])\n",
    "            # passing this modifications to our overall modifications matrices\n",
    "            for l in range(net.nLayers-1):\n",
    "                nablaB[l] += deltaNablaB[l]\n",
    "                nablaW[l] += deltaNablaW[l]\n",
    "        \n",
    "        # applying the changes to our network\n",
    "        for l in range(net.nLayers-1):\n",
    "            net.b[l] = net.b[l] - learningRate * (nablaB[l]/batchSize)\n",
    "            net.w[l] = net.w[l] - learningRate * (nablaW[l]/batchSize)\n",
    "        acc, outputs = testNetwork(net, X, y, nTests=batchSize)\n",
    "        if acc > bestAcc:\n",
    "            bestAcc = acc\n",
    "            bestEpoch = epoch\n",
    "            bestNet = net\n",
    "        print(f'learningRate: {learningRate} epochs: {epoch} acc: {acc}, outputs: {outputs}')\n",
    "    print(f'best acc: {bestAcc} on epoch: {bestEpoch}')\n",
    "    return bestNet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38ebf075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initializing the network and the dataset\n",
    "\n",
    "net = Network([784,30,10])\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5918a0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate: 10 epochs: 0 acc: 0.12, outputs: [17.  3. 11.  1.  9.  9. 13. 32.  3.  2.]\n",
      "learningRate: 10 epochs: 1 acc: 0.14, outputs: [33.  3. 10.  0.  3.  8. 13. 24.  4.  2.]\n",
      "learningRate: 10 epochs: 2 acc: 0.16, outputs: [32.  3. 12.  0.  4. 11. 13. 20.  3.  2.]\n",
      "learningRate: 10 epochs: 3 acc: 0.15, outputs: [17.  3. 16.  0.  6. 13. 15. 22.  5.  3.]\n",
      "learningRate: 10 epochs: 4 acc: 0.15, outputs: [20.  3. 16.  0.  6. 14. 10. 21.  7.  3.]\n",
      "learningRate: 10 epochs: 5 acc: 0.12, outputs: [16.  3. 19.  1.  7. 15. 10. 15. 11.  3.]\n",
      "learningRate: 10 epochs: 6 acc: 0.13, outputs: [12.  5. 19.  2.  9. 14. 10. 22.  4.  3.]\n",
      "learningRate: 10 epochs: 7 acc: 0.12, outputs: [21.  3. 19.  1.  8. 16.  9. 15.  5.  3.]\n",
      "learningRate: 10 epochs: 8 acc: 0.12, outputs: [21.  4. 16.  1. 13. 17.  6. 12.  7.  3.]\n",
      "learningRate: 10 epochs: 9 acc: 0.1, outputs: [19.  4. 16.  1. 17. 16.  4. 12.  8.  3.]\n",
      "learningRate: 10 epochs: 10 acc: 0.09, outputs: [ 7.  7. 18.  2. 19. 17.  4. 15.  8.  3.]\n",
      "learningRate: 10 epochs: 11 acc: 0.11, outputs: [ 8.  7.  8.  1. 26. 17.  4. 18.  8.  3.]\n",
      "learningRate: 10 epochs: 12 acc: 0.1, outputs: [12.  7. 15.  2. 13. 24.  4. 12.  8.  3.]\n",
      "learningRate: 10 epochs: 13 acc: 0.13, outputs: [25.  4. 15.  2.  3. 21.  5. 15.  6.  4.]\n",
      "learningRate: 10 epochs: 14 acc: 0.15, outputs: [29.  4. 14.  2.  5. 22.  4. 15.  2.  3.]\n",
      "learningRate: 10 epochs: 15 acc: 0.17, outputs: [31.  3. 14.  2.  5. 21.  4. 14.  2.  4.]\n",
      "learningRate: 10 epochs: 16 acc: 0.16, outputs: [36.  3. 12.  2.  5. 20.  3. 13.  2.  4.]\n",
      "learningRate: 10 epochs: 17 acc: 0.16, outputs: [44.  1.  8.  2.  4. 26.  1. 10.  1.  3.]\n",
      "learningRate: 10 epochs: 18 acc: 0.17, outputs: [32.  2.  6.  2.  3. 33.  2. 16.  2.  2.]\n",
      "learningRate: 10 epochs: 19 acc: 0.15, outputs: [27.  3.  5.  2.  3. 37.  2. 17.  2.  2.]\n",
      "learningRate: 10 epochs: 20 acc: 0.18, outputs: [36.  3.  6.  2.  3. 26.  3. 16.  2.  3.]\n",
      "learningRate: 10 epochs: 21 acc: 0.18, outputs: [43.  1.  9.  2.  4. 21.  2. 13.  1.  4.]\n",
      "learningRate: 10 epochs: 22 acc: 0.19, outputs: [40.  2.  6.  2.  4. 25.  2. 15.  1.  3.]\n",
      "learningRate: 10 epochs: 23 acc: 0.19, outputs: [48.  1.  7.  1.  4. 18.  1. 16.  1.  3.]\n",
      "learningRate: 10 epochs: 24 acc: 0.2, outputs: [37.  2.  6.  2.  4. 23.  3. 18.  2.  3.]\n",
      "learningRate: 10 epochs: 25 acc: 0.22, outputs: [44.  1.  9.  2.  4. 13.  2. 20.  1.  4.]\n",
      "learningRate: 10 epochs: 26 acc: 0.22, outputs: [37.  2.  6.  2.  3. 17.  3. 24.  2.  4.]\n",
      "learningRate: 10 epochs: 27 acc: 0.21, outputs: [42.  1.  6.  2.  3. 14.  2. 25.  1.  4.]\n",
      "learningRate: 10 epochs: 28 acc: 0.22, outputs: [38.  2.  5.  1.  2. 14.  2. 30.  3.  3.]\n",
      "learningRate: 10 epochs: 29 acc: 0.22, outputs: [44.  1.  5.  1.  3. 15.  2. 24.  2.  3.]\n",
      "learningRate: 10 epochs: 30 acc: 0.21, outputs: [24.  4.  7.  2.  6.  9.  4. 35.  5.  4.]\n",
      "learningRate: 10 epochs: 31 acc: 0.23, outputs: [42.  1.  5.  2.  4.  9.  3. 30.  1.  3.]\n",
      "learningRate: 10 epochs: 32 acc: 0.23, outputs: [52.  1.  5.  1.  5.  5.  2. 26.  0.  3.]\n",
      "learningRate: 10 epochs: 33 acc: 0.23, outputs: [28.  2.  5.  2. 10.  8.  3. 35.  4.  3.]\n",
      "learningRate: 10 epochs: 34 acc: 0.23, outputs: [37.  1.  6.  2.  4.  6.  4. 34.  2.  4.]\n",
      "learningRate: 10 epochs: 35 acc: 0.25, outputs: [29.  2.  4.  2. 10.  8.  5. 34.  3.  3.]\n",
      "learningRate: 10 epochs: 36 acc: 0.26, outputs: [40.  1.  3.  1.  9.  6.  3. 32.  2.  3.]\n",
      "learningRate: 10 epochs: 37 acc: 0.23, outputs: [42.  1.  3.  1.  1.  4.  3. 40.  1.  4.]\n",
      "learningRate: 10 epochs: 38 acc: 0.23, outputs: [28.  3.  3.  2.  2.  6.  5. 44.  3.  4.]\n",
      "learningRate: 10 epochs: 39 acc: 0.25, outputs: [32.  2.  4.  2.  3.  7.  5. 38.  3.  4.]\n",
      "learningRate: 10 epochs: 40 acc: 0.25, outputs: [28.  1.  2.  1.  7.  5.  5. 45.  3.  3.]\n",
      "learningRate: 10 epochs: 41 acc: 0.24, outputs: [34.  1.  3.  1.  1.  4.  4. 46.  2.  4.]\n",
      "learningRate: 10 epochs: 42 acc: 0.26, outputs: [39.  1.  5.  2.  3.  6.  4. 33.  3.  4.]\n",
      "learningRate: 10 epochs: 43 acc: 0.25, outputs: [34.  1.  3.  1.  3.  5.  5. 41.  2.  5.]\n",
      "learningRate: 10 epochs: 44 acc: 0.27, outputs: [33.  1.  6.  1. 12.  8.  4. 26.  4.  5.]\n",
      "learningRate: 10 epochs: 45 acc: 0.29, outputs: [35.  1.  3.  0.  5. 11.  4. 33.  3.  5.]\n",
      "learningRate: 10 epochs: 46 acc: 0.26, outputs: [26.  2.  1.  0.  5.  8.  4. 43.  6.  5.]\n",
      "learningRate: 10 epochs: 47 acc: 0.27, outputs: [34.  1.  2.  1.  3.  7.  4. 40.  4.  4.]\n",
      "learningRate: 10 epochs: 48 acc: 0.25, outputs: [29.  2.  2.  0.  2.  4.  4. 48.  5.  4.]\n",
      "learningRate: 10 epochs: 49 acc: 0.31, outputs: [31.  2.  3.  1.  9.  9.  4. 30.  6.  5.]\n",
      "learningRate: 10 epochs: 50 acc: 0.3, outputs: [38.  1.  1.  1.  7.  8.  4. 34.  3.  3.]\n",
      "learningRate: 10 epochs: 51 acc: 0.32, outputs: [27.  3.  0.  0. 10. 11.  4. 36.  6.  3.]\n",
      "learningRate: 10 epochs: 52 acc: 0.31, outputs: [30.  3.  0.  0.  8. 11.  4. 33.  6.  5.]\n",
      "learningRate: 10 epochs: 53 acc: 0.3, outputs: [22.  2.  1.  0. 10.  9.  7. 37.  7.  5.]\n",
      "learningRate: 10 epochs: 54 acc: 0.29, outputs: [25.  1.  2.  0.  6. 10.  6. 37.  9.  4.]\n",
      "learningRate: 10 epochs: 55 acc: 0.31, outputs: [26.  1.  1.  0. 12.  8.  6. 33.  9.  4.]\n",
      "learningRate: 10 epochs: 56 acc: 0.3, outputs: [35.  2.  3.  0. 12.  3.  5. 28.  6.  6.]\n",
      "learningRate: 10 epochs: 57 acc: 0.27, outputs: [26.  2.  1.  0. 15.  1.  7. 37.  6.  5.]\n",
      "learningRate: 10 epochs: 58 acc: 0.31, outputs: [30.  2.  6.  0. 12.  2. 10. 24.  6.  8.]\n",
      "learningRate: 10 epochs: 59 acc: 0.29, outputs: [29.  2.  4.  0.  8.  3.  7. 34.  6.  7.]\n",
      "learningRate: 10 epochs: 60 acc: 0.29, outputs: [29.  4.  3.  0. 11.  3.  6. 33.  5.  6.]\n",
      "learningRate: 10 epochs: 61 acc: 0.29, outputs: [29.  3.  1.  0. 10.  5.  7. 33.  5.  7.]\n",
      "learningRate: 10 epochs: 62 acc: 0.29, outputs: [30.  2.  2.  0.  8.  5.  7. 33.  6.  7.]\n",
      "learningRate: 10 epochs: 63 acc: 0.27, outputs: [33.  4.  2.  0.  3.  6.  7. 32.  6.  7.]\n",
      "learningRate: 10 epochs: 64 acc: 0.31, outputs: [23.  3.  0.  0.  7. 12.  4. 38.  7.  6.]\n",
      "learningRate: 10 epochs: 65 acc: 0.3, outputs: [16.  4.  0.  0. 30. 13.  4. 22.  6.  5.]\n",
      "learningRate: 10 epochs: 66 acc: 0.28, outputs: [20.  4.  1.  0.  4. 19.  4. 33. 10.  5.]\n",
      "learningRate: 10 epochs: 67 acc: 0.3, outputs: [23.  4.  1.  0.  8. 24.  5. 21.  9.  5.]\n",
      "learningRate: 10 epochs: 68 acc: 0.3, outputs: [23.  4.  0.  0. 11. 28.  5. 18.  6.  5.]\n",
      "learningRate: 10 epochs: 69 acc: 0.33, outputs: [23.  5.  1.  0. 13. 12.  5. 28.  7.  6.]\n",
      "learningRate: 10 epochs: 70 acc: 0.34, outputs: [21.  5.  1.  0. 22. 11.  5. 22.  7.  6.]\n",
      "learningRate: 10 epochs: 71 acc: 0.32, outputs: [23.  7.  1.  0. 15. 17.  5. 22.  6.  4.]\n",
      "learningRate: 10 epochs: 72 acc: 0.31, outputs: [18.  9.  1.  0. 16. 18.  6. 22.  6.  4.]\n",
      "learningRate: 10 epochs: 73 acc: 0.34, outputs: [19.  6.  0.  0. 37.  9.  4. 18.  4.  3.]\n",
      "learningRate: 10 epochs: 74 acc: 0.31, outputs: [28. 10.  0.  0.  2. 12.  6. 31.  6.  5.]\n",
      "learningRate: 10 epochs: 75 acc: 0.31, outputs: [30. 13.  0.  0.  6. 18.  6. 17.  2.  8.]\n",
      "learningRate: 10 epochs: 76 acc: 0.37, outputs: [31. 14.  0.  0. 20.  6.  4. 19.  0.  6.]\n",
      "learningRate: 10 epochs: 77 acc: 0.36, outputs: [24. 16.  0.  0. 20.  4.  4. 27.  0.  5.]\n",
      "learningRate: 10 epochs: 78 acc: 0.37, outputs: [27. 15.  0.  0. 20.  5.  6. 19.  2.  6.]\n",
      "learningRate: 10 epochs: 79 acc: 0.36, outputs: [32. 14.  0.  0. 17.  7.  5. 22.  0.  3.]\n",
      "learningRate: 10 epochs: 80 acc: 0.39, outputs: [22. 17.  0.  0. 17.  9.  6. 27.  0.  2.]\n",
      "learningRate: 10 epochs: 81 acc: 0.38, outputs: [25. 19.  0.  0. 30.  8.  2. 15.  0.  1.]\n",
      "learningRate: 10 epochs: 82 acc: 0.39, outputs: [25. 22.  1.  0. 12.  8. 10. 18.  2.  2.]\n",
      "learningRate: 10 epochs: 83 acc: 0.44, outputs: [22. 26.  0.  0. 23.  8.  4. 17.  0.  0.]\n",
      "learningRate: 10 epochs: 84 acc: 0.41, outputs: [22. 17.  0.  0. 17.  9. 11. 22.  0.  2.]\n",
      "learningRate: 10 epochs: 85 acc: 0.4, outputs: [18. 19.  0.  0. 25. 14.  7. 14.  2.  1.]\n",
      "learningRate: 10 epochs: 86 acc: 0.41, outputs: [25. 18.  0.  0. 15. 16.  9. 16.  0.  1.]\n",
      "learningRate: 10 epochs: 87 acc: 0.45, outputs: [22. 28.  0.  0. 16. 15.  5. 13.  1.  0.]\n",
      "learningRate: 10 epochs: 88 acc: 0.42, outputs: [22. 19.  0.  0. 18.  9.  8. 21.  2.  1.]\n",
      "learningRate: 10 epochs: 89 acc: 0.45, outputs: [22. 23.  0.  0. 14. 16.  8. 16.  1.  0.]\n",
      "learningRate: 10 epochs: 90 acc: 0.38, outputs: [19. 19.  1.  0.  8. 11.  7. 23. 11.  1.]\n",
      "learningRate: 10 epochs: 91 acc: 0.43, outputs: [20. 18.  0.  0. 16. 15.  5. 18.  7.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate: 10 epochs: 92 acc: 0.42, outputs: [26. 19.  2.  0. 12.  9.  7. 17.  6.  2.]\n",
      "learningRate: 10 epochs: 93 acc: 0.46, outputs: [23. 22.  0.  0. 16. 12.  9. 16.  1.  1.]\n",
      "learningRate: 10 epochs: 94 acc: 0.46, outputs: [22. 28.  0.  0. 13. 14.  3. 18.  2.  0.]\n",
      "learningRate: 10 epochs: 95 acc: 0.45, outputs: [24. 24.  0.  0. 16. 11.  4. 17.  3.  1.]\n",
      "learningRate: 10 epochs: 96 acc: 0.44, outputs: [21. 15.  1.  0. 13. 23.  5. 16.  6.  0.]\n",
      "learningRate: 10 epochs: 97 acc: 0.44, outputs: [24. 17.  1.  0.  9.  9.  7. 28.  3.  2.]\n",
      "learningRate: 10 epochs: 98 acc: 0.46, outputs: [22. 25.  0.  0. 14. 13.  4. 20.  2.  0.]\n",
      "learningRate: 10 epochs: 99 acc: 0.49, outputs: [22. 25.  0.  0. 22. 11.  4. 14.  2.  0.]\n",
      "learningRate: 10 epochs: 100 acc: 0.46, outputs: [23. 24.  0.  0. 14. 10.  6. 21.  2.  0.]\n",
      "learningRate: 10 epochs: 101 acc: 0.47, outputs: [22. 20.  0.  0. 17. 13.  8. 15.  4.  1.]\n",
      "learningRate: 10 epochs: 102 acc: 0.48, outputs: [21. 10.  0.  0. 16. 17.  8. 19.  7.  2.]\n",
      "learningRate: 10 epochs: 103 acc: 0.49, outputs: [20. 14.  0.  0. 15. 19.  9. 18.  5.  0.]\n",
      "learningRate: 10 epochs: 104 acc: 0.48, outputs: [21. 16.  0.  0. 15. 17.  8. 17.  6.  0.]\n",
      "learningRate: 10 epochs: 105 acc: 0.47, outputs: [20. 17.  0.  0. 17. 17.  8. 16.  5.  0.]\n",
      "learningRate: 10 epochs: 106 acc: 0.47, outputs: [13. 30.  1.  1. 12. 10.  8. 20.  5.  0.]\n",
      "learningRate: 10 epochs: 107 acc: 0.52, outputs: [17. 27.  0.  1. 14.  9. 13. 16.  3.  0.]\n",
      "learningRate: 10 epochs: 108 acc: 0.52, outputs: [17. 23.  0.  3. 16.  8. 16. 15.  2.  0.]\n",
      "learningRate: 10 epochs: 109 acc: 0.52, outputs: [19. 21.  0.  1. 16. 11. 15. 15.  2.  0.]\n",
      "learningRate: 10 epochs: 110 acc: 0.51, outputs: [23. 26.  0.  1. 15.  9.  8. 16.  2.  0.]\n",
      "learningRate: 10 epochs: 111 acc: 0.54, outputs: [20. 26.  0.  1. 16.  9. 10. 16.  2.  0.]\n",
      "learningRate: 10 epochs: 112 acc: 0.55, outputs: [19. 16.  0.  2. 17. 14. 15. 14.  3.  0.]\n",
      "learningRate: 10 epochs: 113 acc: 0.54, outputs: [16. 13.  0.  1. 18. 15. 22. 13.  2.  0.]\n",
      "learningRate: 10 epochs: 114 acc: 0.52, outputs: [12. 12.  0.  1. 17. 13. 29. 15.  1.  0.]\n",
      "learningRate: 10 epochs: 115 acc: 0.6, outputs: [15. 19.  0.  6. 19.  6. 16. 14.  5.  0.]\n",
      "learningRate: 10 epochs: 116 acc: 0.51, outputs: [17. 16.  3.  2.  5.  7. 15. 30.  5.  0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-ee363b518591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgridSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-f3885e489e9a>\u001b[0m in \u001b[0;36mgridSearch\u001b[1;34m(net, train_X, train_y, test_X, test_y, batchSize, learningRates, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# resetting the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbestAcc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-30ed24765871>\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(net, X, y, batchSize, nEpochs, learningRate)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# finding what should be modified based on this particular example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mdeltaNablaB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltaNablaW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# passing this modifications to our overall modifications matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnLayers\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-b5b1f636d275>\u001b[0m in \u001b[0;36mbackProp\u001b[1;34m(net, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnablaB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mnablaW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 10 because its the possible number of outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-b5b1f636d275>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnablaB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mnablaW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 10 because its the possible number of outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = gridSearch(net, train_X, train_y, test_X, test_y, batchSize=100, learningRates=[10], epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f632dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testNetwork(net, test_X, test_y, len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing our network in action\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pick a sample to plot\n",
    "sample = 50099\n",
    "image = train_X[sample]\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "net = setInput(net, train_X[sample])\n",
    "networkOutput = np.argmax(net.a[-1])\n",
    "networkOutput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
