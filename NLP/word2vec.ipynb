{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f98bdc96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# dataset imported from\n",
    "twitters = pd.read_csv(r'C:\\Users\\Yann\\Desktop\\gotTwitter.csv', usecols=['text'])\n",
    "twitters\n",
    "\n",
    "vocabSize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "65f18631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, vocabSize=vocabSize):\n",
    "    ppt = []\n",
    "\n",
    "    for text in texts:\n",
    "        # removing everything after @\n",
    "        filteredText = re.sub(r'@\\S+', \"\", text);\n",
    "        # removing URLS\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        # removing everything that is not a letter or space\n",
    "        filteredText = re.sub(r'[^\\w ]', u'', filteredText)\n",
    "        filteredText = re.sub(r'[^a-zA-Z\\s]', u'', filteredText, flags=re.UNICODE)\n",
    "        # removing trailing whitespaces and \\n\n",
    "        filteredText = re.sub(' +', ' ', filteredText)\n",
    "        filteredText = filteredText.strip()\n",
    "        filteredText = filteredText.replace('\\n', '')\n",
    "        # lower casing\n",
    "        ppt.append(filteredText.lower())\n",
    "\n",
    "    # keeping only the first vocabSize words of the text\n",
    "    # first finding the vocabSize first elements\n",
    "    t = ' '.join(ppt)\n",
    "    t = t.split(' ')\n",
    "    d = dict()\n",
    "    for word in t:\n",
    "        d[word] = 0\n",
    "    for word in t:\n",
    "        d[word] += 1\n",
    "    wordCount = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "    # making a dictionary of words to keep or to discard (discarded words will be 0 in the dic)\n",
    "    wordsKept = dict()\n",
    "    i = 0\n",
    "    for word in wordCount:\n",
    "        if i < vocabSize:\n",
    "            wordsKept[word] = word\n",
    "        else:\n",
    "            wordsKept[word] = '0'\n",
    "        i += 1\n",
    "    vocabText = []\n",
    "    for sentence in ppt:\n",
    "        t = sentence.split(' ')\n",
    "        s = []\n",
    "        for word in t:\n",
    "            s.append(wordsKept[word])\n",
    "        vocabText.append(' '.join(s))\n",
    "    # returning the preprocessed text with only the first vocabSize most frequent words\n",
    "    # returning the count of that vocabulary\n",
    "    return vocabText, take(vocabSize, wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "63cd0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2int(texts):\n",
    "    t = ' '.join(texts)\n",
    "    t = t.split(' ')\n",
    "    d = dict([(y,x+1) for x,y in enumerate(sorted(set(t)))])\n",
    "    intTexts = []\n",
    "    for sentence in texts:\n",
    "        t = []\n",
    "        for word in sentence:\n",
    "            if word == ' ':\n",
    "                continue\n",
    "            t.append(d[word])\n",
    "        intTexts.append(t)\n",
    "    dS = {v: k for k, v in d.items()}\n",
    "    return intTexts, d, dS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "55217202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shouldKeepWord(word, sample, wordCount, corpusSize):\n",
    "    # if its a word out of the vocabulary\n",
    "    if word == '0':\n",
    "        return False\n",
    "    \n",
    "    n = wordCount[word]\n",
    "    z = n/corpusSize\n",
    "    p = (np.sqrt(z/sample) + 1) * sample/z\n",
    "    \n",
    "    r = np.random.uniform(0,1,1)\n",
    "    if p > r:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4073186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(processedText, vocabSize, wordCount, ISdic, windowSize=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    # the position in the oneHotToken of a word will be it's integer representation\n",
    "    # for example: word \"table\" -> SIdic['table'] = 15 -> oneHotTokens[15] represent the oneHotToken of table\n",
    "    oneHotTokens = np.zeros((vocabSize, vocabSize))\n",
    "    np.fill_diagonal(oneHotTokens, 1)\n",
    "    \n",
    "    corpusSize = 0\n",
    "    for word, count in wordCount.items():\n",
    "        corpusSize += count\n",
    "    for sentence in processedText:\n",
    "        s = []\n",
    "        \n",
    "        # implementing subsampling of frequent words\n",
    "        for word in sentence:\n",
    "            if shouldKeepWord(ISdic[word], 0.001, wordCount, corpusSize):\n",
    "                s.append(word)\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            centerWord = s[i]\n",
    "            j = i\n",
    "            while j < i + windowSize and j < len(sentence)-1:\n",
    "                j += 1\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "            j = i\n",
    "            while j > i - windowSize and j >= 1:\n",
    "                j -= 1\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "        if len(X) > 10000000:\n",
    "            break\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d31a5",
   "metadata": {},
   "source": [
    "Formula for table: $P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum \\limits _{j=0}^{n}(f(w_j)^{3/4})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "237e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTableForNegSampling(wordCount, tableSize=100000000):\n",
    "    # creating a 100M size table \n",
    "    table = np.array(tableSize*[0], dtype=object)\n",
    "    \n",
    "    probabilities = dict()\n",
    "    powSum = 0\n",
    "    for word, count in wordCount.items():\n",
    "        powSum += pow(count, 3/4)\n",
    "        \n",
    "    # calculating the probabilities for each word\n",
    "    for word, count in wordCount.items():\n",
    "        probabilities[word] = pow(count, 3/4)/ powSum\n",
    "    currentIndex = 0\n",
    "    for word, prob in probabilities.items():\n",
    "        nIndexes = int(prob * tableSize)\n",
    "        for i in range(currentIndex, currentIndex + nIndexes):\n",
    "            table[i] = word\n",
    "        \n",
    "        currentIndex += nIndexes\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8535cde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'take' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-a36223bd08c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mppt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-176-2fbe439c34b5>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(texts, vocabSize)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# returning the preprocessed text with only the first vocabSize most frequent words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# returning the count of that vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocabText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'take' is not defined"
     ]
    }
   ],
   "source": [
    "ppt, wordCount = preprocess(twitters['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c871d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating the String to Int and Int to String dictionarys\n",
    "intTweets, SIdic, ISdic = words2int(ppt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae307a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = getTrainingData(intTweets, vocabSize, wordCount, ISdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57428e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = createTableForNegSampling(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de731e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run neuralNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7683d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ec9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randomWords = np.random.randint(len(table), size=5)\n",
    "negSamples = []\n",
    "for i in randomWords:\n",
    "    # finding the int representation of the word and the position on the oneHotVec\n",
    "    print(table[i], i)\n",
    "    negSamples.append(SIdic[table[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb712a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the network has negative sampling\n",
    "net = Network([vocabSize, 300, vocabSize])\n",
    "batchSize = 100\n",
    "nEpochs = len(X)//batchSize\n",
    "net = SGD(net, X, y, nNegSamples=5, unigramTable=table, SIdic=SIdic, batchSize=batchSize, nEpochs=nEpochs, learningRate=1, lamb=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
