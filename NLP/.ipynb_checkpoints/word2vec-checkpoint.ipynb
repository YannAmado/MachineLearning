{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f98bdc96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# dataset imported from\n",
    "twitters = pd.read_csv(r'C:\\Users\\Yann\\Desktop\\gotTwitter.csv', usecols=['text'])\n",
    "twitters\n",
    "\n",
    "vocabSize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65f18631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, vocabSize=vocabSize):\n",
    "    ppt = []\n",
    "\n",
    "    for text in texts:\n",
    "        # removing everything after @\n",
    "        filteredText = re.sub(r'@\\S+', \"\", text);\n",
    "        # removing URLS\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        # removing everything that is not a letter or space\n",
    "        filteredText = re.sub(r'[^\\w ]', u'', filteredText)\n",
    "        filteredText = re.sub(r'[^a-zA-Z\\s]', u'', filteredText, flags=re.UNICODE)\n",
    "        # removing trailing whitespaces and \\n\n",
    "        filteredText = re.sub(' +', ' ', filteredText)\n",
    "        filteredText = filteredText.strip()\n",
    "        filteredText = filteredText.replace('\\n', '')\n",
    "        # lower casing\n",
    "        ppt.append(filteredText.lower())\n",
    "\n",
    "    # keeping only the first vocabSize words of the text\n",
    "    # first finding the vocabSize first elements\n",
    "    t = ' '.join(ppt)\n",
    "    t = t.split(' ')\n",
    "    d = dict()\n",
    "    for word in t:\n",
    "        d[word] = 0\n",
    "    for word in t:\n",
    "        d[word] += 1\n",
    "    wordCount = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "    # making a dictionary of words to keep or to discard (discarded words will be 0 in the dic)\n",
    "    wordsKept = dict()\n",
    "    i = 0\n",
    "    for word in wordCount:\n",
    "        if i < vocabSize:\n",
    "            wordsKept[word] = word\n",
    "        else:\n",
    "            wordsKept[word] = '0'\n",
    "        i += 1\n",
    "    vocabText = []\n",
    "    for sentence in ppt:\n",
    "        t = sentence.split(' ')\n",
    "        s = []\n",
    "        for word in t:\n",
    "            s.append(wordsKept[word])\n",
    "        vocabText.append(' '.join(s))\n",
    "    # creating a new dict that keeps only vocabSize words\n",
    "    i = 0\n",
    "    smallWordCount = dict()\n",
    "    for word, count in wordCount.items():\n",
    "        smallWordCount[word] = count\n",
    "        i += 1\n",
    "        if i >= vocabSize:\n",
    "            break\n",
    "    # returning the preprocessed text with only the first vocabSize most frequent words\n",
    "    # returning the count of that vocabulary\n",
    "    return vocabText, smallWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63cd0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2int(texts):\n",
    "    t = ' '.join(texts)\n",
    "    t = t.split(' ')\n",
    "    d = dict([(y,x+1) for x,y in enumerate(sorted(set(t)))])\n",
    "    intTexts = []\n",
    "    for sentence in texts:\n",
    "        t = []\n",
    "        for word in sentence:\n",
    "            if word == ' ':\n",
    "                continue\n",
    "            t.append(d[word])\n",
    "        intTexts.append(t)\n",
    "    dS = {v: k for k, v in d.items()}\n",
    "    return intTexts, d, dS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55217202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shouldKeepWord(word, sample, wordCount, corpusSize):\n",
    "    # if its a word out of the vocabulary\n",
    "    if word == '0':\n",
    "        return False\n",
    "    \n",
    "    n = wordCount[word]\n",
    "    z = n/corpusSize\n",
    "    p = (np.sqrt(z/sample) + 1) * sample/z\n",
    "    \n",
    "    r = np.random.uniform(0,1,1)\n",
    "    if p > r:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4073186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(processedText, vocabSize, wordCount, ISdic, windowSize=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    # the position in the oneHotToken of a word will be it's integer representation\n",
    "    # for example: word \"table\" -> SIdic['table'] = 15 -> oneHotTokens[15] represent the oneHotToken of table\n",
    "    oneHotTokens = np.zeros((vocabSize, vocabSize))\n",
    "    np.fill_diagonal(oneHotTokens, 1)\n",
    "    \n",
    "    corpusSize = 0\n",
    "    for word, count in wordCount.items():\n",
    "        corpusSize += count\n",
    "    for sentence in processedText:\n",
    "        s = []\n",
    "        \n",
    "        # implementing subsampling of frequent words\n",
    "        for word in sentence:\n",
    "            if shouldKeepWord(ISdic[word], 0.001, wordCount, corpusSize):\n",
    "                s.append(word)\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            centerWord = s[i]\n",
    "            j = i\n",
    "            while j < i + windowSize and j < len(sentence)-1:\n",
    "                j += 1\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "            j = i\n",
    "            while j > i - windowSize and j >= 1:\n",
    "                j -= 1\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "        if len(X) > 10000000:\n",
    "            break\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d31a5",
   "metadata": {},
   "source": [
    "Formula for table: $P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum \\limits _{j=0}^{n}(f(w_j)^{3/4})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "237e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTableForNegSampling(wordCount, tableSize=100000000):\n",
    "    # creating a 100M size table \n",
    "    table = np.array(tableSize*[0], dtype=object)\n",
    "    \n",
    "    probabilities = dict()\n",
    "    powSum = 0\n",
    "    for word, count in wordCount.items():\n",
    "        powSum += pow(count, 3/4)\n",
    "        \n",
    "    # calculating the probabilities for each word\n",
    "    for word, count in wordCount.items():\n",
    "        probabilities[word] = pow(count, 3/4)/ powSum\n",
    "    currentIndex = 0\n",
    "    for word, prob in probabilities.items():\n",
    "        nIndexes = int(prob * tableSize)\n",
    "        for i in range(currentIndex, currentIndex + nIndexes):\n",
    "            table[i] = word\n",
    "        \n",
    "        currentIndex += nIndexes\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8535cde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppt, wordCount = preprocess(twitters['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3c871d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating the String to Int and Int to String dictionarys\n",
    "intTweets, SIdic, ISdic = words2int(ppt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ae307a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = getTrainingData(intTweets, vocabSize, wordCount, ISdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "314c4228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = createTableForNegSampling(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de731e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run neuralNet.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5acab",
   "metadata": {},
   "source": [
    "I need to get the backprop + setInput time down \n",
    "I need to put less words and less features because its taking too long\n",
    "Right now it takes 10s to run one setInput and backProp (2,5s setInput and 7,5s backProp)\n",
    "If i get it down to 0.1s and keep the X length to 1M it will take 28h to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e731a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zone'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISdic[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "add4bfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate: 1 epochs: 0 error: 0.015436433573430509, outputs: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-15dda9a76887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnEpochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnNegSamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigramTable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSIdic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSIdic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-3340e495beb3>\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(net, X, y, nNegSamples, unigramTable, SIdic, batchSize, nEpochs, learningRate, lamb)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# finding what should be modified based on this particular example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mdeltaNablaB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltaNablaW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnNegSamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigramTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSIdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;31m# passing this modifications to our overall modifications matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnLayers\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-6193435d5689>\u001b[0m in \u001b[0;36mbackProp\u001b[1;34m(net, y, nNegSamples, unigramTable, SIdic)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# in the book it needs a transpose because its weight[layer][receivingNeuron][givingNeuron]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# but my implementation uses weight[layer][givingNeuron][receivingNeuron] so it's not necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnablaB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnablaW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the network has negative sampling\n",
    "net = Network([vocabSize, 300, vocabSize])\n",
    "batchSize = 100\n",
    "nEpochs = len(X)//batchSize\n",
    "net = SGD(net, X, y, nNegSamples=5, unigramTable=table, SIdic=SIdic, batchSize=batchSize, nEpochs=nEpochs, learningRate=1, lamb=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
