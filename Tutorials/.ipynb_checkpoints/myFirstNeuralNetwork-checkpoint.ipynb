{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d629ab",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a intuitive guide made for those who are just now getting into neural networks, the objective is to go step by step into how to build a neural network, making sure each and every step is fully understandable. We will be following the book made by Michael Nielsen (link to the book: http://neuralnetworksanddeeplearning.com/index.html), and will try to make the code easier to understand, since a few steps might get lost when going from the book into the code\n",
    "\n",
    "We will be working with the Hello World of neural Networks, The MNIST Dataset, which is a collection of handwritten digits as well as the respective labels (1, 2, 3...). The network will try to correctly classify the digits, and the accuracy will be displayed and used as a measure to how well the network did.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7433b43",
   "metadata": {},
   "source": [
    "# Necessary libraries:\n",
    "\n",
    "If you dont have any of the libraries necessary, just run the following commands on your windows/linux terminal:\n",
    "* Numpy: !pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f871516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8098c0",
   "metadata": {},
   "source": [
    "# First Step: Creating the network\n",
    "\n",
    "In order to create a network, we will be taking advantage of python's classes.\n",
    "\n",
    "If you dont know what a class is, I suggest you take a look at the following link: https://www.w3schools.com/python/python_classes.asp\n",
    "\n",
    "## Why use classes?\n",
    "\n",
    "Classes are just an easy form to make the code easier to use, aggregating meaningful attributes into a single object.\n",
    "In this manner, we are able to more easily track each attribute of the network.\n",
    "\n",
    "\n",
    "### Explaining necessary attributes:\n",
    "\n",
    "In order to create a neural network, a few attributes will be necessary:\n",
    "\n",
    "* number of inputs (nInputs): How many input neurons will the network create. For this simple NN (Neural Network), each pixel will represent an input neuron. The MNIST images have each 28x28 pixels, thus, we will have 28x28 input neurons\n",
    "* number of HiddenLayers (nHiddenLayers): How many hiddend layers should the network have. For a simple NN, only 1 hidden layer will suffice, but for more complex projects, multiple hidden layers may be used (a network with multiple hidden layers is called a *deep neural network* ). The purpose of the hidden layers is to try and \"catch\" patterns of the image, with each subsequent layer bulding upon the last; so for example, a network with 3 hidden layers could work like the following: the first hl (hidden layer) catches the edges, the second turn the edges into shapes (like circles, squares, triangles), and the third turn the shapes into more complex shapes (like eyes, mouth, etc).\n",
    "* number of Neurons per Hidden Layer: How many neurons will each hidden layer have. This is kind of an arbitrary choice, for this network we will be using 30 hidden neurons, simply because this is what Michael Nielsen did in his book. \n",
    "* number of output neurons (nOutputs): How many output Neurons should the network have. It's a good idea for each outcome to have a neuron specific to it, and since we are trying to classify 10 different digits (0 to 9), we will be using 10 output neurons\n",
    "\n",
    "Aside from those attributes which the user will give to the network, other attributes will be made based on those initial ones.\n",
    "### Weights: \n",
    "The weights between two neurons from different layers. For simplicity, we are using numpy to build the matrices with each index assuming a random value between -5 and 5. \n",
    "The matrix we need to build is a 3D matrix. 3D matrix might be scary at first, but our code was structured to have an intuitive manner of accessing each component:\n",
    "* The first index is which layer the \"weight layer\" we are dealing with. A \"weight layer\" is simply the layer of weights between two neurons\n",
    "* The second index is from what neuron the weight is exiting from\n",
    "* The third index is to what neuron the weight is arriving\n",
    "\n",
    "### Bias: \n",
    "For the bias, a 2D matrix will suffice. Like with the weights, the biases will also be started randomly\n",
    "* The first index tells which layer the bias belongs to\n",
    "* The second index tells which neuron the bias belongs to\n",
    "\n",
    "The following code will create the class with all atributes necessary\n",
    "\n",
    "### Activation:\n",
    "For the activation of the neurons, a 2D matrix will also suffice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "57e2eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will be using a fixed seed as to make our network behave equally between runs\n",
    "np.random.seed(9)\n",
    "class Network:\n",
    "    def __init__(self, nInputs, nHiddenLayers, nNeuronsPerHL, nOutputs):\n",
    "        totalLayers = nHiddenLayers + 2 # total layers = (one input+ nHiddenLayers+ one output)\n",
    "        self.totalLayers = totalLayers\n",
    "        self.nInputs = nInputs\n",
    "        self.nHiddenLayers = nHiddenLayers\n",
    "        self.nNeuronsPerHL = nNeuronsPerHL\n",
    "        self.nOutputs = nOutputs\n",
    "        \n",
    "        #initializing the weights and biases randomly using a gaussian distribution with mean 0 and standard deviation 1\n",
    "        sizes = [nInputs]\n",
    "        for i in range(nHiddenLayers):\n",
    "            sizes.append(nNeuronsPerHL)\n",
    "        sizes.append(nOutputs)\n",
    "        self.biases = [np.random.normal(loc=0, scale=1, size=s) for s in sizes[1:None]]                    #which layer, which neuron\n",
    "        \n",
    "        self.weights =[np.random.normal(loc=0, scale=1, size=(x,y)) for x,y in zip(sizes[:None], sizes[1:])]\n",
    "        #which layer, which neuron, which weight\n",
    "        #to access the 1st weight layer 5th exiting neuron 10th arriving neuron: network.weights[0][4][9]\n",
    "        #+1 because the number of weight layers is the total number of layers - 1\n",
    "        self.zActivations = [np.zeros(s) for s in sizes[1:None]]\n",
    "        self.activations = [np.zeros(s) for s in sizes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef50af5",
   "metadata": {},
   "source": [
    "# The sigmoid function\n",
    "In order to our activation neurons on the hidden layers to not explode (meaning, not have an exagerated value, which in turn would mean an exagerated importance); we implement the sigmoid function. It has a few nice traits that are of use to us and is simple enough that can be implement easily, the book has more details about our choice (chapter 1 in the section sigmoid neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "00d222db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(number):\n",
    "    sigNumber = 1/(1 + np.exp(-number))\n",
    "    return sigNumber  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873b791",
   "metadata": {},
   "source": [
    "# Feeding Forward\n",
    "Now, for the bread and butter of neural networks, the feeding forward step. Feeding forward is one of the most fundamental steps of neural networks, and consists of passing information from previous layers to later layers. Combining weights, activations and biases, our neurons will be able to try and find patterns and relative importances on the input, with each passing layer finding (hopefuly) increasingly complex patterns.\n",
    "In pratical terms, the logic is fairly simple: Starting from the input layer, all neurons from the previous layer will pass information to ALL neurons of the next layer, making the iconic shape that represents neural networks. \n",
    "If you dont understand (or wanta a nice visual explanation), I suggest this video of 3blue1brown explaining how it works: https://youtu.be/aircAruvnKk?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=274\n",
    "\n",
    "## Explaining the logic of our code:\n",
    "The logic is as follow:\n",
    "* Starting from the input layer, find the number of neurons of the giving layer and of the receiving (next) layer\n",
    "* Then, loop through every neuron of the receiving layer in the following manner: First, get the bias by inputing the layer and position of the  neuron. Second, loop through every neuron of the previous layers, combining the activation with its respective weight, and summing the results.\n",
    "* Finally, change layers, with the receiving layer now being the giving layer, and the receiving layer being the next one after the current layer.\n",
    "* Repeat until there are no more receiving layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aa38901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(network: Network):\n",
    "    givingLayer = 0\n",
    "    # looping until the last layer\n",
    "    while givingLayer < network.totalLayers-1:                 \n",
    "        if givingLayer == 0:                           #if its the input layer\n",
    "            nGivingNeurons = network.nInputs\n",
    "            nReceivingNeurons = network.nNeuronsPerHL\n",
    "        elif givingLayer == network.totalLayers-2: #if its the layer before the output layer\n",
    "            nGivingNeurons = network.nNeuronsPerHL\n",
    "            nReceivingNeurons = network.nOutputs\n",
    "        else:                                          #if its any layer inbetween\n",
    "            nGivingNeurons = network.nNeuronsPerHL\n",
    "            nReceivingNeurons = network.nNeuronsPerHL\n",
    "        currentLayer = 0\n",
    "        #for each neuron in the layer being fed    \n",
    "        for receivingNeuron in range(nReceivingNeurons):\n",
    "            #the activation of the current neuron is its own biases + all the weights*activations of the previous layer\n",
    "            activation = network.biases[currentLayer][receivingNeuron]\n",
    "            for givingNeuron in range(nGivingNeurons):\n",
    "                activation += network.weights[givingLayer][givingNeuron][receivingNeuron]*network.activations[givingLayer][givingNeuron]\n",
    "            network.activations[givingLayer+1][receivingNeuron] = sigmoid(activation)\n",
    "            \n",
    "        givingLayer += 1\n",
    "        currentLayer += 1\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8f2b07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(network: Network):\n",
    "    lastLayer = network.totalLayers-1\n",
    "    maxIndex = np.argmax(network.activations[lastLayer])\n",
    "    return maxIndex, network.activations[lastLayer][maxIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fa6d6",
   "metadata": {},
   "source": [
    "# Getting the MNIST Dataset\n",
    "There are several ways of getting the MNIST Dataset, I used the keras library just because it seemed the easiest, but feel free to choose what you prefer.\n",
    "If you dont really want to look for an alternative way, use the following commands to install the keras library (keep in mid that the tensorflow library needed is fairly large):\n",
    "* !pip install tensorflow\n",
    "* !pip install keras.\n",
    "\n",
    "Using keras, our data will already be split, between training and testing. If you did it any other way, just make sure that roughly 25% of the images goes to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1345fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487baa92",
   "metadata": {},
   "source": [
    "# Passing images through the network\n",
    "Note that it may take a few minutes, depending on your hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ae3755c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-6b47f22b6631>:2: RuntimeWarning: overflow encountered in exp\n",
      "  sigNumber = 1/(1 + np.exp(-number))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "network = Network(28*28, 1, 30, 10)\n",
    "hits = 0\n",
    "misses = 0\n",
    "nTrainingImages = trainX.shape[0]\n",
    "#for all training images\n",
    "for currentImage in range(nTrainingImages):\n",
    "    #passing the inputs to our network\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            \"\"\" In order to go through all input neurons, we have to transform both loops \n",
    "            in just one index, we do that by using the formula 28*i + j\"\"\"\n",
    "            network.activations[0][28*i + j] = trainX[currentImage][i][j]\n",
    "    network = feedforward(network)\n",
    "    numberNetworkThinks, certainty  = classify(network) \n",
    "    if numberNetworkThinks == trainY[currentImage]:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "    #print(f\"Network Classified as : {numberNetworkThinks}, certainty: {certainty}, real number: {trainY[currentImage]}\")\n",
    "acc = hits/misses\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47a97c",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You know have a fully functional, guess generator. Since our network does not have any learning, it's output is no better than a random a guess; in turn, ou accuracy reflects that line of thought (roughly, our network guessed 10% of tests righ). In the next notebook, we will implement learning, and our network will finaly be of use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
